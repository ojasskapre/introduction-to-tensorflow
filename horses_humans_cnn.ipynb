{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the validation set\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training set\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Unzip training set\n",
    "local_zip = './horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./horse-or-human')\n",
    "\n",
    "# Unzip validation set\n",
    "local_zip = './validation-horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./validation-horse-or-human')\n",
    "\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training horse images: 500\n",
      "total training human images: 527\n",
      "total validation horse images: 128\n",
      "total validation human images: 128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory with training horse pictures\n",
    "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
    "\n",
    "# Directory with training human pictures\n",
    "train_human_dir = os.path.join('./horse-or-human/humans')\n",
    "\n",
    "# Directory with validation horse pictures\n",
    "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
    "\n",
    "# Directory with validation human pictures\n",
    "validation_human_dir = os.path.join('./validation-horse-or-human/humans')\n",
    "\n",
    "print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n",
    "print(f'total training human images: {len(os.listdir(train_human_dir))}')\n",
    "print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n",
    "print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET HORSES: ['horse43-5.png', 'horse06-5.png', 'horse20-6.png', 'horse04-7.png', 'horse41-7.png', 'horse22-4.png', 'horse19-2.png', 'horse24-2.png', 'horse37-8.png', 'horse02-1.png']\n",
      "TRAIN SET HUMANS: ['human17-22.png', 'human10-17.png', 'human10-03.png', 'human07-27.png', 'human09-22.png', 'human05-22.png', 'human02-03.png', 'human02-17.png', 'human15-27.png', 'human12-12.png']\n",
      "VAL SET HORSES: ['horse1-204.png', 'horse2-112.png', 'horse3-498.png', 'horse5-032.png', 'horse5-018.png', 'horse1-170.png', 'horse5-192.png', 'horse1-411.png', 'horse4-232.png', 'horse3-070.png']\n",
      "VAL SET HUMANS: ['valhuman04-20.png', 'valhuman03-01.png', 'valhuman04-08.png', 'valhuman03-15.png', 'valhuman01-04.png', 'valhuman01-10.png', 'valhuman01-11.png', 'valhuman01-05.png', 'valhuman03-14.png', 'valhuman03-00.png']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n",
    "\n",
    "validation_horse_names = os.listdir(validation_horse_dir)\n",
    "print(f'VAL SET HORSES: {validation_horse_names[:10]}')\n",
    "\n",
    "validation_human_names = os.listdir(validation_human_dir)\n",
    "print(f'VAL SET HUMANS: {validation_human_names[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
    "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
    "next_human_pix = [os.path.join(train_human_dir, fname) \n",
    "                for fname in train_human_names[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a model with 5 convolutions and pooling layers and then flatten the result to feed it to the dense layers. Since it is a binary classification we have 1 neuron in output layer with sigmoid activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, 298, 298, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPooli  (None, 149, 149, 16)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 147, 147, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPooli  (None, 73, 73, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 71, 71, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPooli  (None, 35, 35, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 33, 33, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPooli  (None, 16, 16, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPooli  (None, 7, 7, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1704097 (6.50 MB)\n",
      "Trainable params: 1704097 (6.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training dataset using ImageDataGenerator. Loads training images of horse and humans from the directory and resizes them to 300x300 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Found 256 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(300, 300),  # All images will be resized to 300x300\n",
    "        batch_size=128,\n",
    "        # Since you use binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 128 using validation_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        './validation-horse-or-human/',  # This is the source directory for validation images\n",
    "        target_size=(300, 300),  # All images will be resized to 300x300\n",
    "        batch_size=32,\n",
    "        # Since you use binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the dataset for 15 epochs. Since we have 1024 images with batch size 128, steps per epoch is 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 37s 4s/step - loss: 0.8261 - accuracy: 0.5840 - val_loss: 0.6533 - val_accuracy: 0.5352\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 38s 5s/step - loss: 0.6582 - accuracy: 0.5996 - val_loss: 0.7955 - val_accuracy: 0.6992\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 33s 4s/step - loss: 0.7176 - accuracy: 0.6485 - val_loss: 0.6177 - val_accuracy: 0.6797\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 37s 5s/step - loss: 0.4370 - accuracy: 0.8142 - val_loss: 0.9216 - val_accuracy: 0.8398\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 32s 4s/step - loss: 0.2431 - accuracy: 0.9010 - val_loss: 1.7295 - val_accuracy: 0.7734\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 37s 5s/step - loss: 0.3656 - accuracy: 0.8532 - val_loss: 0.7932 - val_accuracy: 0.8906\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 38s 5s/step - loss: 0.4951 - accuracy: 0.8454 - val_loss: 1.6646 - val_accuracy: 0.7656\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 37s 4s/step - loss: 0.1590 - accuracy: 0.9388 - val_loss: 2.0959 - val_accuracy: 0.7969\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 36s 4s/step - loss: 0.2143 - accuracy: 0.9077 - val_loss: 1.7327 - val_accuracy: 0.7930\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 33s 4s/step - loss: 0.1043 - accuracy: 0.9544 - val_loss: 2.0661 - val_accuracy: 0.7969\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 35s 4s/step - loss: 0.4241 - accuracy: 0.8832 - val_loss: 2.0324 - val_accuracy: 0.6836\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 36s 4s/step - loss: 0.1222 - accuracy: 0.9561 - val_loss: 1.8414 - val_accuracy: 0.8047\n",
      "Epoch 13/15\n",
      "8/8 [==============================] - 34s 4s/step - loss: 0.0710 - accuracy: 0.9756 - val_loss: 1.4358 - val_accuracy: 0.8320\n",
      "Epoch 14/15\n",
      "8/8 [==============================] - 32s 4s/step - loss: 0.0274 - accuracy: 0.9922 - val_loss: 1.7446 - val_accuracy: 0.8594\n",
      "Epoch 15/15\n",
      "8/8 [==============================] - 31s 4s/step - loss: 0.1239 - accuracy: 0.9700 - val_loss: 1.5575 - val_accuracy: 0.8477\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=15,\n",
    "      verbose=1,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model prediction. Upload your images to test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n",
      "[1.1186804e-05]\n",
      "horse-197199_640.jpg is a horse\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[1.]\n",
      "white-horse-1136093_640.jpg is a human\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[0.99999523]\n",
      "woman-332278_640.jpg is a human\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[0.999944]\n",
      "entrepreneur-593371_640.jpg is a human\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[1.3994224e-15]\n",
      "horse-2572051_640.jpg is a horse\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "filenames = os.listdir(\"./horse-or-humans-test-images\")\n",
    "\n",
    "for fname in filenames:\n",
    "  # predicting images\n",
    "  path = os.path.join('./horse-or-humans-test-images', fname)\n",
    "  img = load_img(path, target_size=(300, 300))\n",
    "  x = img_to_array(img)\n",
    "  x /= 255\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(classes[0])\n",
    "    \n",
    "  if classes[0]>0.5:\n",
    "    print(fname + \" is a human\")\n",
    "  else:\n",
    "    print(fname + \" is a horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing intermediate representations of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# Define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepare a random input image from the training set.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
    "x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n",
    "\n",
    "# Scale by 1/255\n",
    "x /= 255\n",
    "\n",
    "# Run the image through the network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so you can have them as part of the plot\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Display the representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = feature_map.shape[1]\n",
    "    \n",
    "    # Tile the images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "      # Tile each filter into this big horizontal grid\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    \n",
    "    # Display the grid\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
